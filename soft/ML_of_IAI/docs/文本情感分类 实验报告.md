# 文本情感分类 实验报告

无07 2020010816 陈宇阳

## TextCNN

### 结构图&流程

此模型基本仿照一下结构实现，只是每个卷积核对应的层数为20层。

![img](https://pic4.zhimg.com/80/v2-aa555c351d9a2bff11bff0da0b4c56ff_720w.jpg)

每一个batch中的输入是可变长度的词向量数组`[token_num,50]`，词向量维度为50维，卷积核对应`[kernel_size,50]`，即只在第零维上进行扫描和特征提取，提取后经过`ReLu`层激活，再进行最大池化取出每个kernel每一层对应的最大特征值进行拼接以后得到形如`[kernel_num*kernel_layer,1]`的列向量，最后经过一层全连接层配合`dropout`层和`softmax`层进行拟合和输出即可。

### 实验结果

测试集正确率：84.011%，F-score: 0.831

![image-20220517231607354](..\assets\image-20220517231607354.png)

### 参数比较

| 卷积层层数 | 卷积核（数量及大小） | 准确率 |
| ---------- | -------------------- | ------ |
| 5          | 2，3，4，5，6，7     | 79.01% |
| 20         | 2，3，4，5           | 80.21% |
| 20         | 2，3，4，5，6，7     | 81.02% |
| 30         | 2，3，4，5，6，7     | 82.11% |
| 256        | 2，3，4，5，6，7     | 84.01% |

可见卷积层的层数以及卷积核的数量会微弱地影响特征提取的效果，整体上说层数增加和卷积核增加都会提高模型的准确率，但相应地网络的复杂度和计算所需的资源也增加了。而过多的层数以及卷积核数量也会导致过拟合等反作用。卷积层的层数以及卷积核数量的增加都可以增加网络的特征提取能力，直观地说，更多的层数就能记住句子更多的特征，更大的卷积核就能关注句子中更长的短句。

### CNN问题分析

卷积层数太少会降低模型的精度，而层数太多会增加模型的复杂度，卷积核数量太多会导致收敛速度降低，且训练波动大。可以通过添加批量归一化处理梯度弥散的问题，使得模型收敛得更快。

## LSTM

### 结构图&流程

<img src="..\assets\image-20220517134542776.png" alt="image-20220517134542776" style="zoom: 20%;" />

<img src="https://pic2.zhimg.com/80/v2-556c74f0e025a47fea05dc0f76ea775d_720w.jpg" alt="img" style="zoom:67%;" />

首先LSTM是基于双向RNN实现的长短期记忆网络，理论上拥有记忆前后文内容并进行综合分析的能力。

上一次的输入经过LSTM网络后得到$c^{t-1}$（cell state）和$h^{t-1}$(hidden state)两个状态，并传递到本次的输入当中。

$c^{t-1}$是长期记忆缓慢变化的，而$h^{t-1}$是随着不同的节点快速变化的。

$z^f,z^i,z^o$是由拼接向量乘以权重矩阵之后，再通过一个$Sigmoid$激活函数转换成0到1之间的数值，来作为遗忘门、输入门和输出门。而$z$则是将结果通过一个$tanh$激活函数将转换成-1到1之间的值，使用$tanh$是因为这里是将其作为部分输入数据进行激活以得到输出，而非门控信号。

随着信号输入进来，LSTM会先通过遗忘门$z^o$选择性遗忘来自上一次的h和本次的x中的某些数据，再通过记忆门$z^i$选择性记忆上一次h和本次x中的某些数据，将$z^i$和$z^o$相加，结合$c^{t-1}$可以得到本次$c^t$。$z^o$由$h^{t-1}和x^t$经过输出门生成，而本次网络的输出则由$c^{t-1}$和$z^o$共同决定。

考虑到每一个输入的词向量只有50维，我设计的LSTM隐藏层只有一层，其中有128个节点，且为双向循环网络。

每一个batch中的输入格式是[`seq_len`, `embed_dim`]，其中`seq_len = 119`,是对每一个句子进行全零`padding`后得到的矩阵，`embed_dim=50`。输入经过再LSTM中不断传递与循环，得到输出，经过两层全连接层得到最终的输出。

### 实验结果

测试集准确率：80.488%   F-score：0.812

![image-20220517135109344](..\assets\image-20220517135109344.png)

### 参数比较

| 隐藏层节点数 | 隐藏层层数 | 是否为双向网络 | 准确率 |
| ------------ | ---------- | -------------- | ------ |
| 50           | 2          | 是             | 78.48% |
| 128          | 2          | 是             | 80.27% |
| 128          | 1          | 是             | 80.49% |
| 128          | 1          | 否             | 59.36% |

(更大节点数和更深层数的LSTM电脑实在带不动)

隐藏层节点数和隐藏层层数会很轻微地影响神经网络的表现，但很难从中总结出什么显著的结论。但令人惊讶的是，单向的LSTM网络准确率远远低于双向的LSTM网络，甚至低于多层感知机实现的baseline。

单向的LSTM网络只能储存上文的信息，却很难结合上下文进行分析，对于比较复杂多变的电影评论的输入，可能单向网络难以分析出其中的信息。

### LSTM问题分析

* 出乎意料地，在NLP任务中，RNN的表现稍差于CNN，由于RNN不断循环的特性，它比起CNN所需要的运行时间要长很多，但其学习的效果并不算好，让人怀疑是模型设计上出了问题。考虑到输入的句子中有很多词如人名、英文等，是词向量词典所没有记录的内容，这些词全部都以全零向量代替了，同时我还使用了全零的padding以统一输入的格式，这样会导致输入的句子向量矩阵中出现过多的零向量，导致RNN的一部分神经元（尤其是对应靠后的词向量）得不到相应的训练，因此效果较差。

* 我尝试过使用随机数生成的向量替代Word2Vec词典中没有的词向量，但这样子效果并没有提升，反而因为padding的随机向量太多导致了准确率下降。我还尝试了在网络中加入embedding层，并使用Word2Vec作为预训练的词向量去进行初始化，在后续的训练中对这些词向量再进行学习，但同样没有好的效果。因为最终只有正面和负面情感的label，对于学习50维的词向量来说，信息量太小了，而且本身没有记录的词语都是专有名词，其实学习不出什么内容。

​		假如不使用预训练的词向量，直接对随机初始化的embedding层进行训练，文本情感分类的正确率达到74% ，与baseline的效果相接近。

​		*增加了embedding层的网络可以通过`git checkout embedding`在项目的`embedding`分支中找到。

* 理论上cnn只能做到对单个特征进行识别，而不能分析前后两个特征之间的相互关联，rnn由于储存了上下文的信息，应该可以综合分析出多个特征之间的关系，因此rnn的表现会更好。但经过我使用已经训练的模型对自己的输入进行predict，发现rnn也只能识别直白的表达，对于诸如“我本来以为它会让我失望，但没有”这样的输入也完全无法正确识别，可见RNN的模型的优点并没有很好地发挥出来。



## Baseline: MLP

### model

```python
class MLP(nn.Module):
    def __init__(self,args):
        super(MLP, self).__init__()
        self.args = args
        self.n_class = 2
        self.fc1 = nn.Linear(119*50,512)
        self.fc2 = nn.Linear(512,128)
        self.fc3 = nn.Linear(128,32)
        self.fc4 = nn.Linear(32,self.n_class)
    def forward(self,input):
        # shape[batch_size,119,50]
        x = input.view(self.args.batch_size,119*50)
        x = self.fc1(x)
        x = self.fc2(x)
        x = self.fc3(x)
        x = self.fc4(x)
        return x
```

baseline采用了4层全连接层进行直接的拟合，以下是实验结果。

### 实验结果

<img src="..\assets\image-20220517135717737.png" alt="image-20220517135717737" style="zoom:67%;" />

观察验证集的损失函数和准确率，可见准确率一直在74%上下波动，损失也没有稳定地降低，可见简单的多层感知机模型完全不适用于文本情感分类的任务，简单的全连接层参数无法学习到关键的信息。

### 差异分析

由上图可见，由于所选择的batch_size为512且神经元数量足够、网络深度足够，因此MLP在训练集上可以全局地逼近loss最低点。但在验证集上，loss却没有下降，说明MLP所学习到的并不是我们所希望它学习的文本情感的特征，只是找到了该训练集局部最优解，而并非全局最优解，因此效果不佳。

## 问题思考

1. 实验在什么时候停止是最合适的？

   停止时间应该在准确率和运行时间之间取得平衡。我的做法是，设置一个最大epoch，运行的epoch数达到此值停止实验，保存模型。同时，每一个epoch后都会对当前模型使用验证集进行loss、准确率和F-score的测试，若连续两个epoch准确率都发生了下降且loss不再变小，我就可以认为当前训练效果已经达到了局部最优，再进行训练只会导致过拟合，则停止实验，保存模型。

   固定迭代次数的缺点是无法确定在哪里停止，迭代过多浪费时间且导致过拟合，迭代次数过少则无法得到最优解。

2. 实验参数的初始化是怎么做的？

​		本次实验并没有主动进行参数初始化，因为Pytorch会默认对参数进行初始化，如对于conv层使用`kaiming_uniform`初始化。参数初始化的方法还有零均值初始化、高斯分布初始化、正交初始化等方法，但注意不可以对参数使用零初始化，这样会导致所有权重变化相同。而过大或过小的初值会导致网络输出过大或者过小，对于sigmoid、tanh等激活函数会引发梯度小时的问题。

3. 过拟合是深度学习常见的问题，有什么方法可以防止训练过程陷入过拟合？

   正则化、Dropout层和Early-Stopping都可以防止过拟合。

4. CNN、RNN、MLP三者的优缺点。

   MLP的层数增加后，优化函数容易陷入局部最优解，而层数少又无法解决稍微复杂一点的任务。

​		MLP的权值过多，训练比较慢。CNN具有局部链接、权值共享的特点，卷积层的参数比较少，可以轻易处理高维的数据，训练快，而且具有更强的特征抽取能力。

​		MLP的同层神经元之间不会相互传递，无法对时间进行建模。而RNN则可以做到不同时间的输入互相影响，因此可以处理相互关联的输入，但时间轴太长也会导致梯度消失的问题，无法解决长时间的依赖问题，同时参数也比较多，训练也比较慢。

​		



## 心得体会

通过本次实验我更加深入理解了神经网络，学习到了很多调参的技巧，还熟悉了pytorch的使用，可谓收获颇丰。

但对于实验的结果，本次实验的RNN实现我是不满意的，我本以为它会有远优于CNN的表现，但它并没有表现出任何优势，可能这也是因为我第一次搭建RNN网络，对其中的参数和原理理解得不够透彻。

炼丹真的不容易，哪怕已经有那么成熟的框架，让我可以简单地敲一个`LSTM`就实现了相应的RNN网络，简单地`loss.backward`就实现了反向传播，我所构建的网络也很难有令人满意的表现。神经网络的一大缺点正是其参数的隐藏性，你只能让它学习，却不知道它到底学习到了什么，每一个参数对应着什么，因此模型的调整和超参数的调整也变成了碰运气的行为。我尝试了很多方法，希望能把正确率提高哪怕1%，但把模型复杂化最后起到的都是反作用，比如说随机初始化词典中没有的词向量，在网络中加入embedding层对word2vec向量再次进行训练，进行padding，利用正则化和dropout去防止过拟合，或者改变卷积核的大小、层数等等，最后都很难产生显著的影响。可能也是我本身对于网络的原理理解不透彻，很难想象每一个词向量在网络中到底经历了什么，也就很难对症下药。只能说山高路远，未来的炼丹之路，还需要我坚持不懈地学习。

同时，我也体会到了计算资源的弥足珍贵，用CPU来跑机器学习实在是太慢太痛苦了。